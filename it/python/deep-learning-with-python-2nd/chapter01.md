**[What is deep learning?](https://livebook.manning.com/book/deep-learning-with-python-second-edition/chapter-1)

> dlwp2nd01 <> sharklasers.com 123456

本章涵盖：
- 基本概念的高级定义
- 机器学习发展的时间表
- 深度学习日益普及和未来潜力背后的关键因素

过去几年，人工智能一直是媒体大肆炒作的主体。机器学习、深度学习、和人工智能出现在无数的文章中，通常在技术型出版物之外。
我们期许只能聊天机器人的到来、自动驾驶、虚拟助手——有时候被描述得暗淡无光，有时候又很乌托邦，人们的工作将会稀缺，大多数
经济活动将被机器人或人工智能替代。对于未来或目前从事机器学习的人来说，能够识别噪音中的信号非常重要，这样你就能从过度炒作的新闻发布中辨别出改变世界的发展。我们的未来近在眼前，这是一个你可以扮演积极角色的未来:读完这本书，你将成为开发人工智能代理的人之一。所以让我们来解决这些问题:深度学习到目前为止取得了什么成就?它有多重要?我们下一步要去哪里?你应该相信炒作吗？

本章提供有关人工智能，机器学习和深度学习的基本知识。

# 人工智能、机器学习、深度学习

*Artificial intelligence, machine learning, and deep learning*

![](https://drek4537l1klr.cloudfront.net/chollet2/v-4/Figures/ch01-ai-ml-dl.png)

## 人工智能

*Artificial intelligence*

人工智能始于20世纪五十年代，尽管许多甚至是几十年前的潜在思想都在酝酿之中，但“人工智能”终于在1956年成为研究领域，当时达特茅斯学院年轻的数学助理教授约翰·麦卡锡（John McCarthy）举办了夏季研讨会 根据以下建议：

> 这项研究是基于这样的猜想进行的：理论上可以精确地描述学习的每个方面或智力的任何其他特征，从而可以制造出机器来对其进行仿真。 将尝试寻找如何使机器使用语言，形成抽象和概念，解决现在人类所面临的各种问题以及改善自身的方法。 我们认为，如果一个精心挑选的科学家小组在一个夏天共同研究这个问题，则可以在一个或多个问题上取得重大进展。

暑假结束时，研讨会没有完全解决计划要调查的难题。 然而，许多人参加了会议，他们将继续成为该领域的先驱，并掀起了一场至今仍在进行的知识革命。

简而言之，人工智能可以被描述为将通常由人类执行的智能任务自动化的努力。因此，人工智能是一个包含机器学习和深度学习的一般领域，但它也包括更多可能不涉及任何学习的方法。想想看，直到20世纪80年代，大多数人工智能教科书都没有提到学习!例如，早期的国际象棋程序只涉及由程序员编写的硬编码规则，不符合机器学习的标准。实际上，在相当长的一段时间内，大多数专家认为，通过让程序员手工制作足够大的一组显式规则来操纵存储在显式数据库中的知识，可以实现人类水平的人工智能。 这种方法称为符号AI。 从1950年代到1980年代后期，它一直是AI的主要范例。 在1980年代的专家系统热潮中，它达到了顶峰。

尽管象征性AI被证明适合解决定义明确的逻辑问题，例如下棋，但事实证明为解决更复杂，模糊的问题（例如图像分类，语音识别或自然语言翻译）找出明确的规则是很棘手的 。 出现了一种替代AI的新方法：**机器学习**。

## 机器学习

*Machine learning*

在英格兰维多利亚时代，阿达·洛夫莱斯夫人（Lady Ada Lovelace）是查尔斯·巴贝奇（Charles Babbage）的朋友和合作者，查尔斯·巴贝奇（Analytical Engine）是第一个已知的通用机械计算机的发明者。 尽管分析引擎具有远见和超前的时代，但它在1830年代和1840年代设计时并不意味着它是通用计算机，因为尚未发明通用计算的概念。 它仅是一种使用机械操作使数学分析领域的某些计算自动化的方法，因此得名Analytical Engine。 因此，它是早期尝试以齿轮形式对数学运算进行编码的知识分子后裔，例如Pascaline或Leibniz的步进推算器（Pascaline的改良版）。 由Blaise Pascal在1642年（当时19岁！）设计，Pascaline是世界上第一个机械计算器-它可以加，减，乘甚至除数字。

1843年，Ada Lovelace谈到了分析引擎的发明：

> 分析引擎并不指望能产生任何东西。它可以做我们知道的任何事情，它的职责就是协助我们提供我们已经熟悉的东西。

即使有177年的历史视角，洛夫莱斯夫人的观察仍然令人着迷。 通用计算机可以“起源”任何东西，还是总是绑定到完全执行人类完全理解的过程？ 它能有任何最初的想法吗？ 能从经验中学到吗？ 它可以显示创造力吗？

机器学习，一种新的编程范例：

![](https://drek4537l1klr.cloudfront.net/chollet2/v-4/Figures/ch01-a-new-programming-paradigm.png)

机器学习系统是经过训练的，而不是经过明确编程的。 它提供了许多与任务相关的示例，并在这些示例中找到了统计结构，最终使系统能够提出使任务自动化的规则。 例如，如果您希望自动化标记度假照片的任务，则可以展示一个机器学习系统，其中包含许多已经被人类标记过的照片示例，并且该系统将学习将特定图片与特定标签相关联的统计规则。   

尽管机器学习只是在1990年代才开始蓬勃发展，但它已迅速成为AI最受欢迎和最成功的子领域，这一趋势是由更快的硬件和更大的数据集的可用性所驱动的。 机器学习与数学统计紧密相关，但是它在一些重要方面与统计不同。 与统计数据不同，机器学习倾向于处理大型，复杂的数据集（例如，数百万个图像的数据集，每个数据集由数万个像素组成），对于这些数据集，经典的统计分析（如贝叶斯分析）将是不切实际的。 结果，机器学习，尤其是深度学习，展示的数学理论相对较少（可能太少），并且从根本上说是一门工程学科。 与理论物理学或数学不同，机器学习是一个非常动手的领域，它由经验发现驱动，并且高度依赖于软件和硬件的进步。

## 从数据中学习规则和表达

*Learning rules and representations from data*

要定义深度学习并了解深度学习与其他机器学习方法之间的区别，首先我们需要对机器学习算法的功能有所了解。 我们刚刚说过，机器学习发现了执行数据处理任务的规则，并给出了预期的示例。 因此，要进行机器学习，我们需要三件事：

- 输入数据点 —— 例如，如果任务是语音识别，则这些数据点可以是说话人的声音文件。 如果任务是图像标记，则它们可能是图片。
- 预期输出的示例 —— 在语音识别任务中，这些可能是人为生成的声音文件的笔录。 在图像任务中，预期的输出可能是诸如“狗”，“猫”之类的标签。
- 衡量算法是否做得很好的一种方法 —— 这是确定算法当前输出与其预期输出之间的距离所必需的。 该测量用作反馈信号，以调整算法的工作方式。 这个调整步骤就是我们所说的学习。

机器学习模型将其输入数据转换为有意义的输出，该过程是从接触已知的输入和输出示例中“学习”的。 因此，机器学习和深度学习中的中心问题是有意义地转换数据：换句话说，学习手头输入数据的有用表示形式—使我们更接近预期输出的表示形式。

在继续之前：什么是表示？ 本质上，它是查看数据的另一种方法-表示或编码数据。 例如，彩色图像可以RGB格式（红绿蓝）或HSV格式（色相饱和度值）编码：这是同一数据的两种不同表示形式。 有些表示可能很难完成的任务，而另一种表示可能变得很容易。 例如，任务“选择图像中的所有红色像素”在RGB格式中更简单，而“使图像饱和度更低”在HSV格式中更简单。 机器学习模型都是关于为其输入数据找到合适的表示形式的-数据的转换使其更适合手头的任务。

让我们具体一点。考虑一个 x 轴，一个 y 轴，以及一些点在 (x, y) 系统中的坐标:

![](https://drek4537l1klr.cloudfront.net/chollet2/v-4/Figures/ch01-example_data_points.png)

我们有一些百色的点，一些黑色的点，假设我们想要开发一种算法，它可以取一个点的坐标 (x, y) 并输出该点可能是黑的还是白的：

- 输入就是这些点的坐标
- 期望的输出是带颜色的点
- 衡量算法好用与否的一种方式，就是正确分类点的百分比


这里我们需要的是一种新的数据表示，它能清晰地将白点和黑点分开。在许多其他可能性中，我们可以使用的一种转换是坐标变化，如图所示。

![](https://drek4537l1klr.cloudfront.net/chollet2/v-4/Figures/ch01-learning_representations.png)

在新的坐标体系中，知道了对数据新的表示，非常棒！通过这样的表示，黑白分类问题可简单的表达为规则：黑点的 x > 0，白点的 x < 0 。

在本例中，我们手动定义坐标变化:我们使用人类智能来给出我们自己的数据适当表示。对于这样一个极其简单的问题，这是可以的，但是如果任务是对手写数字的图像进行分类，您是否也可以这样做呢?你能写下显式的，计算机可执行的图像转换来说明 6 和 8, 1 和 7 之间的区别吗？

因此，这就是机器学习的本质，简单地说:利用反馈信号的指导，在预定义的可能性空间内，在一些输入数据上搜索有用的表示和规则。这个简单的想法可以解决从语音识别到自动驾驶等一系列非常广泛的智力任务。

## 深度学习中的“深度”

*The “deep” in deep learning*

深度学习是机器学习的一个特定子领域：从数据中学习表示形式的一种新方法，重点在于学习越来越有意义的表示形式的连续层。 深度学习不是指通过这种方法获得的任何更深刻的理解； 相反，它代表了连续表示层的想法。 数据模型中有多少层被称为模型深度。 该字段的其他合适名称可以是分层表示学习和分层表示学习。 现代深度学习通常涉及数十个甚至数百个连续的表示层，并且它们都是通过接触训练数据而自动获得的。 同时，其他机器学习方法倾向于只学习一层或两层数据表示（例如，获取像素直方图，然后应用分类规则）； 因此，它们有时被称为浅层学习。

在深度学习中，这些分层表示(几乎总是)是通过一种叫做神经网络的模型来学习的，这种模型是由相互堆叠的文字层构成的。神经网络这个术语是对神经生物学的一种参考，但尽管深度学习的一些核心概念部分是通过我们对大脑(特别是视觉皮层)的理解而发展起来的，深度学习模型并不是大脑的模型。没有证据表明大脑有现代深度学习模型中使用的学习机制。你可能会遇到一些流行科学文章，声称深度学习的工作原理与大脑相似，或者是模仿大脑，但事实并非如此。对于这个领域的新手来说，如果认为深度学习与神经生物学有任何关系，那将是令人困惑和适得其反的;你不需要像我们的思想那样的神秘感和神秘感，你也可能会忘记你可能读过的关于深度学习和生物学之间假设联系的任何东西。就我们的目的而言，深度学习是一种从数据中学习表示的数学框架。

深度学习算法学习到的表示法是什么样子的?让我们来看看几个层次的网络（见图）是如何转换一个数字的图像来识别它是哪个数字。

![](https://drek4537l1klr.cloudfront.net/chollet2/v-4/Figures/ch01-a_deep_network.png)

网络将数字图像转换为与原始图像越来越不同且对最终结果的信息越来越丰富的表示形式。 您可以将深度网络视为多级信息蒸馏操作，其中信息经过连续的过滤器并逐渐被净化（这对某些任务很有用）。

![](https://drek4537l1klr.cloudfront.net/chollet2/v-4/Figures/ch01-mnist_representations.png)

这就是深度学习，技术上来说:一种多阶段学习数据表示的方法。这是一个简单的想法，但事实证明，非常简单的机制，如果规模足够大，最终看起来就像魔术一样。

## 三张图理解深度学习如何工作

*Understanding how deep learning works, in three figures*

在这一点上，您知道机器学习是关于将输入（例如图像）映射到目标（例如标签“ cat”）的，这是通过观察输入和目标的许多示例来完成的。 您还知道，深层神经网络通过一整套简单的数据转换（层）的深度序列来完成从输入到目标的映射，并且这些数据转换是通过接触示例来学习的。 现在，让我们具体看看这种学习是如何发生的。

图层对输入数据的处理方式的规范存储在图层的权重中，权重本质上是一堆数字。 用技术术语来说，我们可以说由图层实现的转换是由其权重参数化的。 （权重有时也称为层的参数）在这种情况下，学习意味着为网络中所有层的权重找到一组值，以便网络将示例输入正确地映射到其关联目标。 但这就是事实：一个深度神经网络可以包含数千万个参数。 为所有参数找到正确的值似乎是一项艰巨的任务，尤其是考虑到修改一个参数的值会影响其他所有参数的行为！

![](https://drek4537l1klr.cloudfront.net/chollet2/v-4/Figures/ch01-deep-learning-in-3-figures-1.png)

控制某物，首先您需要能够观察它。 要控制神经网络的输出，您需要能够测量该输出与预期的距离。 这是网络损失函数的工作，有时也称为目标函数或成本函数。 损失函数获取网络和真实目标（您希望网络输出的内容）的预测，并计算距离得分，以捕获网络在此特定示例中的表现。

![](https://drek4537l1klr.cloudfront.net/chollet2/v-4/Figures/ch01-deep-learning-in-3-figures-2.png)

深度学习的基本技巧是使用这个分数作为反馈信号，稍微调整权重的值，以降低当前例子中的损失分数。这种调整是优化器的工作，它实现了所谓的反向传播算法:深度学习的中心算法。下一章将更详细地解释反向传播的工作原理。

![](https://drek4537l1klr.cloudfront.net/chollet2/v-4/Figures/ch01-deep-learning-in-3-figures-3.png)

最初，为网络的权重分配了随机值，因此网络仅执行一系列随机变换。 自然，它的输出与理想情况相去甚远，因此损耗分数非常高。 但是在网络处理的每个示例中，权重都在正确的方向上进行了一些调整，并且损失分数降低了。 这是训练循环，该循环重复了足够的次数（通常在数千个示例中进行数十次迭代），从而产生使损失函数最小化的权重值。 损失最小的网络就是这样的网络，它的输出应尽可能接近目标：训练有素的网络。 再说一次，这是一种简单的机制，一旦扩展，最终看起来就像魔术。

## 深度学习已经获得了多大成就

*What deep learning has achieved so far*