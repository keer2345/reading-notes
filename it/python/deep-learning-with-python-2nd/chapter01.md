**[What is deep learning?](https://livebook.manning.com/book/deep-learning-with-python-second-edition/chapter-1)

> dlwp2nd01 <> sharklasers.com 123456

本章涵盖：
- 基本概念的高级定义
- 机器学习发展的时间表
- 深度学习日益普及和未来潜力背后的关键因素

过去几年，人工智能一直是媒体大肆炒作的主体。机器学习、深度学习、和人工智能出现在无数的文章中，通常在技术型出版物之外。
我们期许只能聊天机器人的到来、自动驾驶、虚拟助手——有时候被描述得暗淡无光，有时候又很乌托邦，人们的工作将会稀缺，大多数
经济活动将被机器人或人工智能替代。对于未来或目前从事机器学习的人来说，能够识别噪音中的信号非常重要，这样你就能从过度炒作的新闻发布中辨别出改变世界的发展。我们的未来近在眼前，这是一个你可以扮演积极角色的未来:读完这本书，你将成为开发人工智能代理的人之一。所以让我们来解决这些问题:深度学习到目前为止取得了什么成就?它有多重要?我们下一步要去哪里?你应该相信炒作吗？

本章提供有关人工智能，机器学习和深度学习的基本知识。

- [人工智能、机器学习、深度学习](#人工智能机器学习深度学习)
  - [人工智能](#人工智能)
  - [机器学习](#机器学习)
  - [从数据中学习规则和表达](#从数据中学习规则和表达)
  - [深度学习中的“深度”](#深度学习中的深度)
  - [三张图理解深度学习如何工作](#三张图理解深度学习如何工作)
  - [深度学习已经获得了多大成就](#深度学习已经获得了多大成就)
  - [不要相信短期炒作](#不要相信短期炒作)
  - [人工智能的前景](#人工智能的前景)
- [深度学习之前：机器学习简史](#深度学习之前机器学习简史)
  - [概率建模](#概率建模)
  - [早期的神经网络](#早期的神经网络)
  - [核心方法](#核心方法)
  - [决策树、决策森林和梯度增强机器](#决策树决策森林和梯度增强机器)
  - [回到神经网络](#回到神经网络)
  - [是什么让深度学习与众不同](#是什么让深度学习与众不同)
  - [现代机器学习领域](#现代机器学习领域)
- [为什么需要深度学习？而且是现在](#为什么需要深度学习而且是现在)

# 人工智能、机器学习、深度学习

*Artificial intelligence, machine learning, and deep learning*

![](https://drek4537l1klr.cloudfront.net/chollet2/v-4/Figures/ch01-ai-ml-dl.png)

## 人工智能

*Artificial intelligence*

人工智能始于20世纪五十年代，尽管许多甚至是几十年前的潜在思想都在酝酿之中，但“人工智能”终于在1956年成为研究领域，当时达特茅斯学院年轻的数学助理教授约翰·麦卡锡（John McCarthy）举办了夏季研讨会 根据以下建议：

> 这项研究是基于这样的猜想进行的：理论上可以精确地描述学习的每个方面或智力的任何其他特征，从而可以制造出机器来对其进行仿真。 将尝试寻找如何使机器使用语言，形成抽象和概念，解决现在人类所面临的各种问题以及改善自身的方法。 我们认为，如果一个精心挑选的科学家小组在一个夏天共同研究这个问题，则可以在一个或多个问题上取得重大进展。

暑假结束时，研讨会没有完全解决计划要调查的难题。 然而，许多人参加了会议，他们将继续成为该领域的先驱，并掀起了一场至今仍在进行的知识革命。

简而言之，人工智能可以被描述为将通常由人类执行的智能任务自动化的努力。因此，人工智能是一个包含机器学习和深度学习的一般领域，但它也包括更多可能不涉及任何学习的方法。想想看，直到20世纪80年代，大多数人工智能教科书都没有提到学习!例如，早期的国际象棋程序只涉及由程序员编写的硬编码规则，不符合机器学习的标准。实际上，在相当长的一段时间内，大多数专家认为，通过让程序员手工制作足够大的一组显式规则来操纵存储在显式数据库中的知识，可以实现人类水平的人工智能。 这种方法称为符号AI。 从1950年代到1980年代后期，它一直是AI的主要范例。 在1980年代的专家系统热潮中，它达到了顶峰。

尽管象征性AI被证明适合解决定义明确的逻辑问题，例如下棋，但事实证明为解决更复杂，模糊的问题（例如图像分类，语音识别或自然语言翻译）找出明确的规则是很棘手的 。 出现了一种替代AI的新方法：**机器学习**。

## 机器学习

*Machine learning*

在英格兰维多利亚时代，阿达·洛夫莱斯夫人（Lady Ada Lovelace）是查尔斯·巴贝奇（Charles Babbage）的朋友和合作者，查尔斯·巴贝奇（Analytical Engine）是第一个已知的通用机械计算机的发明者。 尽管分析引擎具有远见和超前的时代，但它在1830年代和1840年代设计时并不意味着它是通用计算机，因为尚未发明通用计算的概念。 它仅是一种使用机械操作使数学分析领域的某些计算自动化的方法，因此得名Analytical Engine。 因此，它是早期尝试以齿轮形式对数学运算进行编码的知识分子后裔，例如Pascaline或Leibniz的步进推算器（Pascaline的改良版）。 由Blaise Pascal在1642年（当时19岁！）设计，Pascaline是世界上第一个机械计算器-它可以加，减，乘甚至除数字。

1843年，Ada Lovelace谈到了分析引擎的发明：

> 分析引擎并不指望能产生任何东西。它可以做我们知道的任何事情，它的职责就是协助我们提供我们已经熟悉的东西。

即使有177年的历史视角，洛夫莱斯夫人的观察仍然令人着迷。 通用计算机可以“起源”任何东西，还是总是绑定到完全执行人类完全理解的过程？ 它能有任何最初的想法吗？ 能从经验中学到吗？ 它可以显示创造力吗？

机器学习，一种新的编程范例：

![](https://drek4537l1klr.cloudfront.net/chollet2/v-4/Figures/ch01-a-new-programming-paradigm.png)

机器学习系统是经过训练的，而不是经过明确编程的。 它提供了许多与任务相关的示例，并在这些示例中找到了统计结构，最终使系统能够提出使任务自动化的规则。 例如，如果您希望自动化标记度假照片的任务，则可以展示一个机器学习系统，其中包含许多已经被人类标记过的照片示例，并且该系统将学习将特定图片与特定标签相关联的统计规则。   

尽管机器学习只是在1990年代才开始蓬勃发展，但它已迅速成为AI最受欢迎和最成功的子领域，这一趋势是由更快的硬件和更大的数据集的可用性所驱动的。 机器学习与数学统计紧密相关，但是它在一些重要方面与统计不同。 与统计数据不同，机器学习倾向于处理大型，复杂的数据集（例如，数百万个图像的数据集，每个数据集由数万个像素组成），对于这些数据集，经典的统计分析（如贝叶斯分析）将是不切实际的。 结果，机器学习，尤其是深度学习，展示的数学理论相对较少（可能太少），并且从根本上说是一门工程学科。 与理论物理学或数学不同，机器学习是一个非常动手的领域，它由经验发现驱动，并且高度依赖于软件和硬件的进步。

## 从数据中学习规则和表达

*Learning rules and representations from data*

要定义深度学习并了解深度学习与其他机器学习方法之间的区别，首先我们需要对机器学习算法的功能有所了解。 我们刚刚说过，机器学习发现了执行数据处理任务的规则，并给出了预期的示例。 因此，要进行机器学习，我们需要三件事：

- 输入数据点 —— 例如，如果任务是语音识别，则这些数据点可以是说话人的声音文件。 如果任务是图像标记，则它们可能是图片。
- 预期输出的示例 —— 在语音识别任务中，这些可能是人为生成的声音文件的笔录。 在图像任务中，预期的输出可能是诸如“狗”，“猫”之类的标签。
- 衡量算法是否做得很好的一种方法 —— 这是确定算法当前输出与其预期输出之间的距离所必需的。 该测量用作反馈信号，以调整算法的工作方式。 这个调整步骤就是我们所说的学习。

机器学习模型将其输入数据转换为有意义的输出，该过程是从接触已知的输入和输出示例中“学习”的。 因此，机器学习和深度学习中的中心问题是有意义地转换数据：换句话说，学习手头输入数据的有用表示形式—使我们更接近预期输出的表示形式。

在继续之前：什么是表示？ 本质上，它是查看数据的另一种方法-表示或编码数据。 例如，彩色图像可以RGB格式（红绿蓝）或HSV格式（色相饱和度值）编码：这是同一数据的两种不同表示形式。 有些表示可能很难完成的任务，而另一种表示可能变得很容易。 例如，任务“选择图像中的所有红色像素”在RGB格式中更简单，而“使图像饱和度更低”在HSV格式中更简单。 机器学习模型都是关于为其输入数据找到合适的表示形式的-数据的转换使其更适合手头的任务。

让我们具体一点。考虑一个 x 轴，一个 y 轴，以及一些点在 (x, y) 系统中的坐标:

![](https://drek4537l1klr.cloudfront.net/chollet2/v-4/Figures/ch01-example_data_points.png)

我们有一些百色的点，一些黑色的点，假设我们想要开发一种算法，它可以取一个点的坐标 (x, y) 并输出该点可能是黑的还是白的：

- 输入就是这些点的坐标
- 期望的输出是带颜色的点
- 衡量算法好用与否的一种方式，就是正确分类点的百分比


这里我们需要的是一种新的数据表示，它能清晰地将白点和黑点分开。在许多其他可能性中，我们可以使用的一种转换是坐标变化，如图所示。

![](https://drek4537l1klr.cloudfront.net/chollet2/v-4/Figures/ch01-learning_representations.png)

在新的坐标体系中，知道了对数据新的表示，非常棒！通过这样的表示，黑白分类问题可简单的表达为规则：黑点的 x > 0，白点的 x < 0 。

在本例中，我们手动定义坐标变化:我们使用人类智能来给出我们自己的数据适当表示。对于这样一个极其简单的问题，这是可以的，但是如果任务是对手写数字的图像进行分类，您是否也可以这样做呢?你能写下显式的，计算机可执行的图像转换来说明 6 和 8, 1 和 7 之间的区别吗？

因此，这就是机器学习的本质，简单地说:利用反馈信号的指导，在预定义的可能性空间内，在一些输入数据上搜索有用的表示和规则。这个简单的想法可以解决从语音识别到自动驾驶等一系列非常广泛的智力任务。

## 深度学习中的“深度”

*The “deep” in deep learning*

深度学习是机器学习的一个特定子领域：从数据中学习表示形式的一种新方法，重点在于学习越来越有意义的表示形式的连续层。 深度学习不是指通过这种方法获得的任何更深刻的理解； 相反，它代表了连续表示层的想法。 数据模型中有多少层被称为模型深度。 该字段的其他合适名称可以是分层表示学习和分层表示学习。 现代深度学习通常涉及数十个甚至数百个连续的表示层，并且它们都是通过接触训练数据而自动获得的。 同时，其他机器学习方法倾向于只学习一层或两层数据表示（例如，获取像素直方图，然后应用分类规则）； 因此，它们有时被称为浅层学习。

在深度学习中，这些分层表示(几乎总是)是通过一种叫做神经网络的模型来学习的，这种模型是由相互堆叠的文字层构成的。神经网络这个术语是对神经生物学的一种参考，但尽管深度学习的一些核心概念部分是通过我们对大脑(特别是视觉皮层)的理解而发展起来的，深度学习模型并不是大脑的模型。没有证据表明大脑有现代深度学习模型中使用的学习机制。你可能会遇到一些流行科学文章，声称深度学习的工作原理与大脑相似，或者是模仿大脑，但事实并非如此。对于这个领域的新手来说，如果认为深度学习与神经生物学有任何关系，那将是令人困惑和适得其反的;你不需要像我们的思想那样的神秘感和神秘感，你也可能会忘记你可能读过的关于深度学习和生物学之间假设联系的任何东西。就我们的目的而言，深度学习是一种从数据中学习表示的数学框架。

深度学习算法学习到的表示法是什么样子的?让我们来看看几个层次的网络（见图）是如何转换一个数字的图像来识别它是哪个数字。

![](https://drek4537l1klr.cloudfront.net/chollet2/v-4/Figures/ch01-a_deep_network.png)

网络将数字图像转换为与原始图像越来越不同且对最终结果的信息越来越丰富的表示形式。 您可以将深度网络视为多级信息蒸馏操作，其中信息经过连续的过滤器并逐渐被净化（这对某些任务很有用）。

![](https://drek4537l1klr.cloudfront.net/chollet2/v-4/Figures/ch01-mnist_representations.png)

这就是深度学习，技术上来说:一种多阶段学习数据表示的方法。这是一个简单的想法，但事实证明，非常简单的机制，如果规模足够大，最终看起来就像魔术一样。

## 三张图理解深度学习如何工作

*Understanding how deep learning works, in three figures*

在这一点上，您知道机器学习是关于将输入（例如图像）映射到目标（例如标签“ cat”）的，这是通过观察输入和目标的许多示例来完成的。 您还知道，深层神经网络通过一整套简单的数据转换（层）的深度序列来完成从输入到目标的映射，并且这些数据转换是通过接触示例来学习的。 现在，让我们具体看看这种学习是如何发生的。

图层对输入数据的处理方式的规范存储在图层的权重中，权重本质上是一堆数字。 用技术术语来说，我们可以说由图层实现的转换是由其权重参数化的。 （权重有时也称为层的参数）在这种情况下，学习意味着为网络中所有层的权重找到一组值，以便网络将示例输入正确地映射到其关联目标。 但这就是事实：一个深度神经网络可以包含数千万个参数。 为所有参数找到正确的值似乎是一项艰巨的任务，尤其是考虑到修改一个参数的值会影响其他所有参数的行为！

![](https://drek4537l1klr.cloudfront.net/chollet2/v-4/Figures/ch01-deep-learning-in-3-figures-1.png)

控制某物，首先您需要能够观察它。 要控制神经网络的输出，您需要能够测量该输出与预期的距离。 这是网络损失函数的工作，有时也称为目标函数或成本函数。 损失函数获取网络和真实目标（您希望网络输出的内容）的预测，并计算距离得分，以捕获网络在此特定示例中的表现。

![](https://drek4537l1klr.cloudfront.net/chollet2/v-4/Figures/ch01-deep-learning-in-3-figures-2.png)

深度学习的基本技巧是使用这个分数作为反馈信号，稍微调整权重的值，以降低当前例子中的损失分数。这种调整是优化器的工作，它实现了所谓的反向传播算法:深度学习的中心算法。下一章将更详细地解释反向传播的工作原理。

![](https://drek4537l1klr.cloudfront.net/chollet2/v-4/Figures/ch01-deep-learning-in-3-figures-3.png)

最初，为网络的权重分配了随机值，因此网络仅执行一系列随机变换。 自然，它的输出与理想情况相去甚远，因此损耗分数非常高。 但是在网络处理的每个示例中，权重都在正确的方向上进行了一些调整，并且损失分数降低了。 这是训练循环，该循环重复了足够的次数（通常在数千个示例中进行数十次迭代），从而产生使损失函数最小化的权重值。 损失最小的网络就是这样的网络，它的输出应尽可能接近目标：训练有素的网络。 再说一次，这是一种简单的机制，一旦扩展，最终看起来就像魔术。

## 深度学习已经获得了多大成就

*What deep learning has achieved so far*

尽管深度学习是机器学习的一个相当老的子领域，但它只是在2010年代初才受到重视。 从那以后的几年中，它在该领域取得了巨大的成就，在感知任务甚至自然语言处理任务方面取得了令人瞩目的成果-这些问题涉及人类似乎自然而直观的技能，但长期以来一直困扰着机器。

特别是，深度学习实现了以下突破，这些突破都是在机器学习的历史难题领域：
- 接近人图像分类
- 接近人类的语音转录
- 接近人类水平的笔迹转录
- 极大地改善了机器翻译
- 极大地改善了文本到语音的转换
- 接近乎人类的自动驾驶
- 改进了Google，百度或Bing使用的广告定位
- 改善网络搜索结果
- 回答自然语言问题的能力
- 超人去玩

我们仍在探索深度学习可以做什么的全部范围。 我们已经成功地将其应用到了几年前被认为无法解决的各种问题上-自动转录了梵蒂冈秘密档案馆中保存的成千上万的古代手稿，对植物病害进行了检测和分类 使用简单的智能手机在野外工作，协助肿瘤科医生或放射科医生解释医学成像数据，预测洪水，飓风甚至地震等自然灾害……每一个里程碑，我们都将接近一个深度学习在每个活动和活动中为我们提供帮助的时代 人类努力的每个领域-科学，医学，制造，能源，运输，软件开发，农业甚至艺术创作。

## 不要相信短期炒作

*Don’t believe the short-term hype*

尽管近年来深度学习已取得了令人瞩目的成就，但对未来十年该领域将能够实现的目标的期望往往远高于可能实现的目标。尽管诸如自动驾驶汽车之类的改变世界的应用已经可以触及，但很长一段时间仍可能难以捉摸，例如可信的对话系统，跨任意语言的人机翻译以及人文自然语言理解。 特别是，关于人的通用情报的谈论不应太当真。 短期内寄予厚望的风险是，由于技术无法交付，研究投资将枯竭，并在很长一段时间内减缓进展。

这种情况以前发生过。过去，人工智能曾两次经历强烈乐观、失望和怀疑的循环，结果是缺乏资金。它始于20世纪60年代的象征性人工智能。在早期，人们对人工智能的预测非常高。象征性人工智能方法最著名的先驱和支持者之一是马文·明斯基(Marvin Minsky)。他在1967年宣称，在一代人的时间内，创造人工智能的问题“将得到实质性的解决”。三年后的1970年，他做出了更为精确的量化预测：“在三到八年内，我们将拥有一台具有普通人一般智慧的机器。” 到2020年，这样的成就似乎仍是遥远的未来，目前我们还无法预测这需要多长时间，但在20世纪60年代和70年代初，一些专家相信它即将到来(现在很多人也这么认为)。几年后，由于这些高期望未能实现，研究人员和政府资金退出了该领域，这标志着第一个人工智能冬天的开始(这里指核冬天，因为这是冷战高峰后不久)。

这不会是最后一个。20世纪80年代，一种新的象征性人工智能——专家系统——开始在大公司中兴起。一些最初的成功案例引发了一波投资热潮，世界各地的企业都成立了自己的人工智能部门来开发专家系统。1985年前后，公司每年在这项技术上的花费超过10亿美元;但到了20世纪90年代初，事实证明，这些系统维护成本高昂，难以扩展，而且范围有限，人们对这些系统的兴趣逐渐消退。第二个人工智能冬天就这样开始了。

我们可能正在经历人工智能炒作和失望的第三个周期，我们仍处于极度乐观的阶段。最好是缓和我们对短期的预期，并确保不太熟悉该领域技术方面的人清楚地知道深度学习可以实现什么，不能实现什么。

## 人工智能的前景

*The promise of AI*

尽管我们可能对人工智能有不切实际的短期期望，但长期前景是光明的。在将深度学习应用于许多重要问题上，我们才刚刚起步，从医疗诊断到数字助理，它可能会被证明是革命性的。AI研究向前发展非常迅速在过去的十年里,在很大程度上由于一定程度的资金从未见过在人工智能的历史很短,但到目前为止,相对较少的进步使得在产品和过程,形成我们的世界。深度学习的大多数研究结果尚未应用，或者至少没有应用到它们可以解决所有行业的所有问题上。 您的医生尚未使用AI，您的会计师也没有。 您在日常生活中可能不会经常使用AI技术。 当然，您可以问您的智能手机简单的问题并获得合理的答案，可以在Amazon.com上获得相当有用的产品推荐，还可以在Google相册中搜索“生日”，然后立即查找上个月和女儿生日聚会的照片。 与此类技术过去的情况相去甚远。 但是这些工具仍然只是我们日常生活的附件。 人工智能尚未转变为我们工作，思考和生活方式的中心。

现在，似乎很难相信AI可能会对我们的世界产生巨大影响，因为它尚未得到广泛部署-就像1995年一样，人们很难相信AI的未来影响 。 那时，大多数人都没有看到互联网与他们之间的关系以及如何改变他们的生活。 今天的深度学习和AI也是这样。 但是请不要误会：AI即将来临。 在不久的将来，人工智能将成为您的助手，甚至是您的朋友。 它会回答您的问题，帮助教育您的孩子，并注意您的健康。 它会将您的食品杂货运送到您的家门，并将您从A点驱逐到B点。它将是您进入日益复杂和信息密集的世界的界面。 而且，更重要的是，人工智能将通过协助人类科学家在从基因组学到数学的所有科学领域中取得突破性发现，来帮助整个人类向前发展。

在这个过程中，我们可能会遇到一些挫折，甚至可能迎来一个新的人工智能冬天，就像互联网行业在1998年至1999年被过度炒作，并在整个21世纪初遭遇了一场导致投资枯竭的崩溃一样。但我们最终会做到的。人工智能最终将被应用于构成我们社会和日常生活的几乎每一个过程，就像今天的互联网一样。

不相信短期炒作，但相信长期愿景。 要使AI发挥出真正的潜力可能需要一段时间，这是一个尚未有人敢梦想的潜力，但是AI即将来临，它将以一种奇妙的方式改变我们的世界。

# 深度学习之前：机器学习简史

*Before deep learning: a brief history of machine learning*

深度学习已经达到了AI历史上从未有过的公众关注和行业投资的水平，但这并不是机器学习的第一种成功形式。 可以肯定地说，当今行业中使用的大多数机器学习算法都不是深度学习算法。 深度学习并非始终是正确的工作工具-有时没有足够的数据可供深度学习使用，并且有时可以通过其他算法更好地解决问题。 如果深度学习是您与机器学习的第一次接触，那么您可能会遇到这样的情况，即只有深度学习的锤子，每个机器学习问题都像钉子一样开始。 不落入此陷阱的唯一方法是熟悉其他方法，并在适当时进行练习。

## 概率建模

*Probabilistic modeling*

概率建模是统计原理在数据分析中的应用。 它是最早的机器学习形式之一，至今仍被广泛使用。 此类别中最著名的算法之一是朴素贝叶斯算法。

朴素贝叶斯(Naive Bayes)是一种基于贝叶斯定理的机器学习分类器，它假设输入数据中的特征都是独立的(一种强的或朴素的假设，这就是这个名字的由来)。这种形式的数据分析比计算机要早，并且在第一次计算机实现之前的几十年(很可能要追溯到20世纪50年代)就已经被手工应用了。贝叶斯定理和统计学基础可以追溯到18世纪，这些就是你开始使用朴素贝叶斯分类器所需要的。

一个密切相关的模型是逻辑回归（loglog回归）（简称logreg），有时被认为是现代机器学习的“世界”。 不要被它的名字所迷惑-logreg是一种分类算法，而不是回归算法。 就像Naive Bayes一样，logreg在很长一段时间内就已经过时了，但是由于其简单和通用的特性，它在今天仍然有用。 数据科学家通常会首先尝试对数据集进行操作，以掌握手头的分类任务。

## 早期的神经网络

*Early neural networks*

这些页面中介绍的现代变体已完全取代了神经网络的早期迭代，但是了解深度学习是如何产生的，这很有帮助。 尽管早在1950年代就以玩具形式研究了神经网络的核心思想，但该方法花了数十年才开始。 长期以来，缺少的部分是训练大型神经网络的有效方法。 这种情况在1980年代中期发生了变化，当时多个人独立地重新发现了反向传播算法-一种使用梯度下降优化来训练参数化运算链的方法（在本书的后面，我们将精确定义这些概念），并开始将其应用于 神经网络。

神经网络的第一个成功的实际应用来自贝尔实验室，当时是Yann LeCun于1989年结合了卷积神经网络和反向传播的早期思想，并将其应用于对手写数字进行分类的问题。 由此产生的被称为LeNet的网络在1990年代被美国邮政服务用于自动读取邮件信封上的邮政编码。

## 核心方法

*Kernel methods*

在1990年代，随着神经网络开始在研究人员中赢得一定的尊重，这得益于其首个成功，一种新的机器学习方法逐渐成名，并迅速使神经网络回到被遗忘的阶段：内核方法。 内核方法是一组分类算法，其中最著名的就是支持向量机（SVM）。 SVM的现代公式是由Vladimir Vapnik和Corinna Cortes在1990年代初期在贝尔实验室开发的，并于1995年发表[3]，尽管更早的线性公式由Vapnik和Alexey Chervonenkis早于1963年发表。

一个决策边界图：

[](https://drek4537l1klr.cloudfront.net/chollet2/v-4/Figures/ch01-boundary.png)

SVM分两步进行查找这些边界：
1. 数据被映射到新的高维表示形式，其中决策边界可以表示为超平面（如果数据是二维的，如上图所示，超平面将是一条直线）。
1. 通过尝试使超平面与每个类中最接近的数据点之间的距离最大化，可以计算出一个好的决策边界（分离超平面），这一步骤称为使裕度最大化。 这使边界可以很好地推广到训练数据集之外的新样本。

在分类问题变得更简单的情况下，将数据映射到高维表示的技术在纸面上看起来不错，但在实践中，它通常在计算上很棘手。 这就是内核技巧的来源（内核方法以其命名的关键思想）。 要点是：要在新的表示空间中找到好的决策超平面，您不必显式计算新空间中的点的坐标； 您只需要计算该空间中成对的点之间的距离即可，这可以使用内核函数有效地完成。 内核函数是一种易于计算的运算，它将初始空间中的任意两个点映射到目标表示空间中这些点之间的距离，从而完全绕开了新表示的显式计算。 内核功能通常是手工制作的，而不是从数据中学习的-在SVM的情况下，仅学习分离超平面。

在开发它们时，SVM在简单的分类问题上表现出最先进的性能，并且是由广泛的理论支持并能进行认真的数学分析的为数不多的机器学习方法之一，使其易于理解和易于理解。 由于这些有用的特性，SVM长期以来在该领域变得非常流行。

但是事实证明，SVM很难扩展到大型数据集，并且在诸如图像分类之类的感知问题上并未提供良好的结果。 由于SVM是一种浅层方法，因此将SVM应用于感知问题需要首先手动提取有用的表示（称为特征工程的步骤），这既困难又脆弱。 例如，如果您想使用SVM对手写数字进行分类，则不能从原始像素开始，您应该首先手动找到有用的表示方法，以使问题更容易解决，例如我们前面提到的像素直方图。

## 决策树、决策森林和梯度增强机器

*Decision trees, random forests, and gradient boosting machines*

决策树是类似于流程图的结构，可让您对输入数据点进行分类或预测给定输入的输出值。 它们很容易可视化和解释。 从数据中学到的决策树在2000年代开始受到广泛的研究兴趣，到2010年，它们通常比内核方法更受青睐。

决策树:学习的参数是关于数据的问题。比如，问题可能是，数据中的系数2是否大于3.5？如图：

![](https://drek4537l1klr.cloudfront.net/chollet2/v-4/Figures/ch01-decision_tree.png)

特别是，随机森林算法引入了一种强大的，实用的决策树学习方法，该方法涉及构建大量专门的决策树，然后将其输出汇总。 随机森林适用于各种各样的问题-您可以说，对于任何浅层机器学习任务，随机森林几乎总是第二好的算法。 当流行的机器学习竞赛网站Kaggle（kaggle.com）于2010年开始运营时，随机森林迅速成为该平台上的最爱-直到2014年，梯度增强机器接管了。 梯度增强机器非常类似于随机森林，是一种基于综合弱预测模型（通常是决策树）的机器学习技术。 它使用梯度提升，这是一种通过迭代训练专门解决先前模型的弱点的新模型来改善任何机器学习模型的方法。 应用于决策树时，使用梯度增强技术可以使模型在大多数情况下严格胜过随机森林，同时具有相似的属性。 它可能是当今处理非感知数据的最好的算法之一，即使不是最好的算法。 除深度学习外，它是Kaggle竞赛中最常用的技术之一。

## 回到神经网络

*Back to neural networks*

010年前后，尽管整个科学界几乎完全避开了神经网络，但仍在研究神经网络的许多人开始取得重要突破：多伦多大学的Geoffrey Hinton组，蒙特利尔大学的Yoshua Bengio组 ，纽约大学的Yann LeCun和瑞士的IDSIA。

2011年，来自IDSIA的Dan Ciresan开始通过GPU训练的深度神经网络赢得学术图像分类竞赛，这是现代深度学习的首次实践成功。 但转折点出现在2012年，Hinton小组参加了每年一次的大规模图像分类挑战赛ImageNet（简称ImageNet大规模视觉识别挑战赛，简称ILSVRC）。 当时，ImageNet的挑战非常困难，包括在对140万张图像进行训练后，将高分辨率彩色图像分为1,000种不同的类别。 2011年，基于经典的计算机视觉方法，获胜模型的前五名准确性仅为74.3％。 然后，在2012年，由Alex Krizhevsky领导并在Geoffrey Hinton的建议下完成的团队实现了83.6％的前五名准确性，这是一个重大突破。 此后，每年的竞争都由深度卷积神经网络主导。 到2015年，获胜者的准确率达到96.4％，ImageNet上的分类任务被认为是一个完全解决的问题。

自2012年以来，深度卷积神经网络（convnet）已成为所有计算机视觉任务的首选算法； 更一般而言，他们从事所有感知任务。 在2015年以后的任何大型计算机视觉会议上，几乎都不可能找到不涉及某种形式的卷积网络的演示。 同时，深度学习还发现了许多其他类型的问题的应用，例如自然语言处理。 它已在各种应用中完全取代了SVM和决策树。 例如，多年来，欧洲核子研究组织欧洲核研究组织使用基于决策树的方法来分析大型强子对撞机（LHC）的ATLAS探测器的粒子数据。 但由于CERN的更高性能和易于在大型数据集上进行训练的能力，最终将其转换为基于Keras的深度神经网络。

## 是什么让深度学习与众不同

*What makes deep learning different*

深度学习之所以如此迅速发展，其主要原因是它可以在许多问题上提供更好的性能。 但这不是唯一的原因。 深度学习还使解决问题变得更加容易，因为它可以完全自动化机器学习工作流程中最关键的步骤：功能工程。

以前的机器学习技术-浅层学习-通常仅通过简单的转换（例如高维非线性投影（SVM）或决策树）将输入数据转换为一个或两个连续的表示空间。 但是，通常无法通过此类技术获得复杂问题所需的精确表示。 因此，人类必须竭尽全力使初始输入数据更适合通过这些方法进行处理：他们必须手动为数据设计良好的表示层。 这称为特征工程。 深度学习则完全自动化了这一步骤：通过深度学习，您可以一次学习所有功能，而不必自己进行设计。 这大大简化了机器学习的工作流程，通常用一个简单，端到端的深度学习模型来代替复杂的多级管道。

您可能会问，如果问题的症结在于要有多个连续的表示层，是否可以重复应用浅层方法来模仿深度学习的效果？ 在实践中，浅层学习方法的连续应用回报迅速减少，因为三层模型中的最佳第一表示层不是一层或两层模型中的最佳第一层。 深度学习具有革命性的意义在于，它允许模型同时而不是连续地学习所有表示层，而不必连续学习（即所谓的贪婪）。 通过联合特征学习，每当模型调整其内部特征之一时，依赖于该模型的所有其他特征都会自动适应更改，而无需人工干预。 一切都由一个反馈信号进行监控：模型中的每一个变化都为最终目标服务。 这比贪婪地堆叠浅层模型要强大得多，因为它允许将复杂的抽象表示分解为长的一系列中间空间（层）来学习。 每个空间仅是对前一个空间的简单转换。

这是深度学习如何从数据中学习的两个基本特征：逐步，逐层地开发越来越复杂的表示形式的方法，以及联合学习这些中间增量表示法的事实，每一层都进行更新以同时遵循 上一层的代表性需求和下一层的需求。 总之，这两个特性使深度学习比以前的机器学习方法更加成功。

## 现代机器学习领域

*The modern machine-learning landscape*

了解Kaggle上的机器学习竞赛的一种很好的方式来了解机器学习算法和工具的当前状况。 由于竞争激烈的环境（有些比赛有成千上万的参赛者和数百万美元的奖金），而且由于涉及到各种各样的机器学习问题，Kaggle提供了一种切实可行的方法来评估什么有效，什么无效。 那么，哪种算法可以可靠地赢得比赛呢？ 最佳进入者使用哪些工具？

在2019年初，Kaggle进行了一项调查，询问自2017年以来在比赛中排名前五名的团队，他们使用了哪些主要软件工具。 事实证明，高层团队倾向于使用深度学习方法（最经常通过Keras库）或梯度增强树（最经常通过LightGBM或XGBoost库）。

![](https://drek4537l1klr.cloudfront.net/chollet2/v-4/Figures/kaggle_top_teams_tools.png)

也不仅仅是比赛冠军。 Kaggle还对全球的机器学习和数据科学专业人员进行了年度调查。 这项调查有成千上万的受访者，是我们有关行业状况的最可靠来源之一。下图显示了不同机器学习软件框架的使用百分比。

![](https://drek4537l1klr.cloudfront.net/chollet2/v-4/Figures/kaggle_ds_survey_2019.png)

从2016年到2020年，整个机器学习和数据科学行业一直被这两种方法所主导：深度学习和梯度增强树。 具体来说，梯度增强树用于存在结构化数据可用的问题，而深度学习用于感知问题，例如图像分类。

梯度增强树的用户倾向于使用Scikit-Learn，XGBoost或LightGBM。 同时，大多数深度学习的从业者经常将Keras与父级框架TensorFlow结合使用。 这些工具的共同点是它们都是Python库：Python是迄今为止机器学习和数据科学中使用最广泛的语言。

为了成功地在当今的应用机器学习中，您应该最熟悉这两种技术：梯度增强树，用于浅层学习的问题； 和深度学习，以解决感性问题。 从技术上讲，这意味着您需要熟悉Scikit-Learn，XGBoost和Keras（目前在Kaggle竞赛中占主导地位的三个库）。 有了这本书，您已经迈出了一大步。

# 为什么需要深度学习？而且是现在

![](Why deep learning? Why now?)


到1990年，计算机视觉深度学习的两个关键思想（卷积神经网络和反向传播）已经得到了很好的理解。长期短期记忆（LSTM）算法是时间序列深度学习的基础，它于1997年开发并具有 从那以后几乎没有改变。 那么为什么深度学习只在2012年之后才开始兴起呢？ 这二十年来发生了什么变化？